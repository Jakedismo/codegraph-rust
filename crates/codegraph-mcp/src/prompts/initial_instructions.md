# CodeGraph MCP Server - Initial Instructions for AI Agents

## Introduction

Welcome to CodeGraph! This document guides you on using CodeGraph's code-intelligence tools effectively during development. These are **soft guidelines**, not rigid rulesâ€”adapt them to your context.

## Core Philosophy

**Evidence-Based Development**: Ground all architectural decisions and code insights in actual codebase analysis, not assumptions.

**Metacognitive Workflow**: Think â†’ Verify â†’ Act. Always explain your reasoning before using tools.

---

## Tool Selection Framework

### ğŸ¯ Decision Gates (Use These Before Tool Calls)

**Gate 1: Scope Clarity**
- â“ "What specific question am I trying to answer?"
- â“ "Do I need broad context or pinpoint precision?"
- âœ… Proceed only when you can state the question in one sentence

**Gate 2: Tool Appropriateness**
- â“ "Which tool directly answers this question?"
- â“ "Am I reaching for the right granularity?" (project â†’ file â†’ symbol)
- âœ… Choose the most specific tool that covers your scope

**Gate 3: Evidence Requirement**
- â“ "What evidence do I need to collect before making a claim?"
- â“ "Can I cite tool outputs to support this conclusion?"
- âœ… Never infer; always verify with tool data

**Gate 4: Safety Check**
- â“ "Could this tool call modify state or be expensive?"
- â“ "Do I understand the parameters I'm passing?"
- âœ… For write operations, explain your intent first

---

## Tool Categories & Selection Criteria

### ğŸ” Category 1: Discovery & Search Tools

**When to use:** Starting analysis, exploring unfamiliar code, finding entry points

**Tools:**
- `enhanced_search` - **Natural language semantic search**
  - âœ… Use when: "How does X work?", "Find implementations of Y"
  - ğŸ“Š Returns: Ranked results with code snippets + explanations
  - ğŸ’¡ Pattern: Start broad, then drill down with specific tools

- `semantic_intelligence` - **Deep pattern analysis**
  - âœ… Use when: Need design patterns, architecture insights, code quality analysis
  - ğŸ“Š Returns: Patterns, anti-patterns, architectural recommendations
  - ğŸ’¡ Pattern: Use after enhanced_search to understand design context

**Workflow Pattern:**
```
1. enhanced_search("authentication flow")
   â†’ Get high-level overview
2. semantic_intelligence on key files
   â†’ Understand patterns and quality
3. Drill down with graph tools for dependencies
```

### ğŸ§  Category 2: Impact & Dependency Analysis Tools

**When to use:** Planning changes, understanding blast radius, refactoring

**Tools:**
- `impact_analysis` - **Change impact prediction**
  - âœ… Use when: Before modifying code, assessing refactoring scope
  - ğŸ“Š Returns: Affected files, functions, tests, risk level
  - âš ï¸ Critical gate: ALWAYS run before major changes
  - ğŸ’¡ Pattern: Run â†’ Review â†’ Plan â†’ Execute

- `graph_neighbors` - **Direct dependencies**
  - âœ… Use when: Need immediate callers/callees of a function
  - ğŸ“Š Returns: 1-hop dependency graph
  - ğŸ’¡ Pattern: Quick check for local impact

- `graph_traverse` - **Transitive dependency chains**
  - âœ… Use when: Tracing call chains, understanding dependency depth
  - ğŸ“Š Returns: Full dependency paths up to max depth
  - ğŸ’¡ Pattern: Use after graph_neighbors for deep analysis

**Workflow Pattern:**
```
Before Refactoring:
1. impact_analysis on target code
   â†’ Understand full scope
2. graph_traverse for dependency chains
   â†’ Map transitive impacts
3. Review affected tests
   â†’ Plan test updates
4. Make changes
5. Re-run impact_analysis
   â†’ Verify no unexpected ripples
```

### ğŸ“Š Category 3: Code Intelligence Tools

**When to use:** Understanding code quality, finding patterns, detecting issues

**Tools:**
- `pattern_detection` - **Anti-pattern & smell detection**
  - âœ… Use when: Code review, quality assessment, finding technical debt
  - ğŸ“Š Returns: Detected patterns, severity, remediation suggestions
  - ğŸ’¡ Pattern: Run on new code before committing

- `vector_search` - **Similarity-based code search**
  - âœ… Use when: "Find code similar to this", "Where else do we do X?"
  - ğŸ“Š Returns: Semantically similar code blocks
  - ğŸ’¡ Pattern: Use for consistency checks and duplicate detection

**Workflow Pattern:**
```
Code Quality Check:
1. pattern_detection on changed files
   â†’ Identify issues
2. vector_search for similar patterns
   â†’ Check consistency across codebase
3. semantic_intelligence for design validation
   â†’ Ensure patterns align with architecture
```

### ğŸ”§ Category 4: Performance & Metrics Tools

**When to use:** Profiling, optimization, system health monitoring

**Tools:**
- `performance_metrics` - **System performance data**
  - âœ… Use when: Investigating slowness, profiling bottlenecks
  - ğŸ“Š Returns: CPU, memory, I/O metrics, bottleneck detection
  - ğŸ’¡ Pattern: Baseline â†’ Change â†’ Measure â†’ Compare

**Workflow Pattern:**
```
Performance Investigation:
1. performance_metrics (baseline)
   â†’ Capture current state
2. Identify hotspots in output
3. Use enhanced_search to find implementation
4. Analyze with semantic_intelligence
5. Make optimization
6. performance_metrics (comparison)
   â†’ Validate improvement
```

---

## Metacognitive Reasoning Patterns

### ğŸ§© Pattern 1: Progressive Refinement

**Principle:** Start broad, narrow iteratively

```
Question: "How does authentication work?"

âŒ Bad: Immediately grep for "authenticate"
âœ… Good:
  1. enhanced_search("authentication system")
     â†’ Understand high-level flow
  2. graph_neighbors on auth entry point
     â†’ See direct dependencies
  3. semantic_intelligence on auth module
     â†’ Understand design patterns
  4. Now I can speak confidently about the system
```

**Reasoning Gate:** "Can I explain the architecture before diving into implementation details?"

### ğŸ§© Pattern 2: Evidence-Driven Claims

**Principle:** Never state facts without tool citations

```
âŒ Bad: "This uses JWT for auth"
âœ… Good: "Based on enhanced_search results showing JwtValidator in auth/tokens.rs:45, the system uses JWT tokens"

âŒ Bad: "Changing this will break tests"
âœ… Good: "impact_analysis shows this change affects 12 test files (listed in output), requiring test updates"
```

**Reasoning Gate:** "Can I cite the specific tool output that supports this claim?"

### ğŸ§© Pattern 3: Impact-First Changes

**Principle:** Understand consequences before acting

```
Before any refactoring:

âœ… Required sequence:
  1. impact_analysis on target code
  2. Review affected components
  3. Explain the blast radius
  4. Get confirmation if impact > expected
  5. Only then proceed

âŒ Skip this at your peril: Unintended breakage
```

**Reasoning Gate:** "Have I mapped all affected code and tests before changing anything?"

### ğŸ§© Pattern 4: Context Layering

**Principle:** Build understanding in layers: Project â†’ Module â†’ Component â†’ Implementation

```
New feature in unfamiliar area:

âœ… Layer 1 (Project): enhanced_search for high-level patterns
âœ… Layer 2 (Module): semantic_intelligence on relevant modules
âœ… Layer 3 (Component): graph_traverse to understand dependencies
âœ… Layer 4 (Implementation): Read specific files with context

Each layer informs the next. Skip layers = miss critical context.
```

**Reasoning Gate:** "Do I understand each layer before going deeper?"

---

## Safety & Best Practices

### ğŸ”’ Hard Requirements (These are non-negotiable)

1. **Impact Analysis Before Refactoring**
   - MUST run `impact_analysis` before modifying shared code
   - MUST review affected files list
   - MUST explain why the impact is acceptable

2. **Evidence-Based Reasoning**
   - MUST cite tool outputs when making claims
   - MUST NOT infer behavior without verification
   - MUST separate "tool output says X" from "I think Y"

3. **Explain Before Execute**
   - MUST explain reasoning before tool calls
   - MUST state what question you're answering
   - MUST NOT chain tools without reviewing intermediate results

### ğŸ’¡ Soft Suggestions (Consider these guidelines)

1. **Start with Search, End with Graph**
   - Typically: enhanced_search â†’ understand â†’ graph tools â†’ verify
   - Use semantic_intelligence for design-level questions
   - Use graph tools when you need hard dependency data

2. **Layer Your Analysis**
   - Consider: Project-level â†’ Module-level â†’ Component-level
   - Avoid: Jumping straight to implementation without context

3. **Iterate on Results**
   - Review tool outputs before next step
   - Refine queries based on what you learn
   - Build mental model incrementally

4. **Validate Assumptions**
   - When uncertain, verify with vector_search for similar code
   - Use pattern_detection to confirm suspected anti-patterns
   - Cross-reference multiple tool outputs for confidence

---

## Common Workflows

### ğŸš€ Workflow: Implementing a New Feature

```
1. Discovery Phase
   enhanced_search("similar feature")
   â†’ Find existing patterns

2. Design Phase
   semantic_intelligence on similar modules
   â†’ Understand design patterns
   graph_traverse on entry points
   â†’ Map integration points

3. Impact Phase
   impact_analysis on files you'll modify
   â†’ Understand change scope

4. Implementation Phase
   Write code
   pattern_detection on new code
   â†’ Ensure quality

5. Validation Phase
   Run tests
   performance_metrics (if relevant)
   â†’ Verify no regressions
```

### ğŸ”§ Workflow: Debugging an Issue

```
1. Locate Phase
   enhanced_search("error symptom")
   â†’ Find relevant code

2. Context Phase
   graph_neighbors on suspected function
   â†’ See what calls it
   semantic_intelligence on the file
   â†’ Understand design intent

3. Root Cause Phase
   graph_traverse backwards from symptom
   â†’ Trace the bug path
   vector_search for similar bugs
   â†’ Check if pattern exists elsewhere

4. Fix Phase
   impact_analysis on fix location
   â†’ Ensure fix is safe
   Make change
   Re-run search to verify fix
```

### ğŸ“š Workflow: Learning Unfamiliar Code

```
1. Overview Phase
   enhanced_search("high-level question")
   â†’ Get architectural overview

2. Structure Phase
   semantic_intelligence on main modules
   â†’ Understand design patterns

3. Dependency Phase
   graph_traverse from entry points
   â†’ Map call flows

4. Detail Phase
   Read specific files with context
   Use vector_search for examples
   â†’ Understand implementation
```

---

## Tool Parameters: What to Provide

### Quality Gates for Parameters

**Before calling ANY tool:**
- âœ… "Are my parameters specific enough?"
- âœ… "Am I using appropriate filters?"
- âœ… "Do I understand what each parameter does?"

### Parameter Guidelines by Tool

**enhanced_search:**
- `query`: Natural language, specific question
- `top_k`: Start with 5-10, increase if needed
- ğŸ’¡ Tip: Phrase as a question, not keywords

**semantic_intelligence:**
- `file_path` or `code_snippet`: Be specific
- Use file paths when analyzing whole modules
- Use code snippets for targeted analysis

**impact_analysis:**
- `file_path` + `line_range` OR `symbol_name`
- âš ï¸ Critical: Provide accurate scope
- Review output before changes

**graph_neighbors / graph_traverse:**
- `node_id` or `symbol_name`: Exact symbol reference
- `max_depth`: Start small (2-3), increase if needed
- ğŸ’¡ Tip: Use enhanced_search first to find node IDs

**pattern_detection:**
- `file_paths` or `code_snippet`
- Consider scope: Single file vs directory
- ğŸ’¡ Tip: Run on modified files before committing

**vector_search:**
- `query`: Code snippet or description
- `top_k`: 5-10 for precision, 20+ for exploration
- ğŸ’¡ Tip: Use for "find similar" questions

**performance_metrics:**
- `operation`: Specific operation to profile
- Use baseline measurements for comparisons

---

## Red Flags: When You're Off Track

**ğŸš¨ Stop and reconsider if:**

- You're about to change code without running `impact_analysis`
- You're making claims without citing tool outputs
- You're using tools randomly hoping for insights
- You haven't explained your reasoning before tool calls
- You're skipping from discovery to implementation without understanding design
- You're ignoring tool warnings or unexpected results

**âœ… You're on track if:**

- You can explain why you chose each tool
- Each tool call answers a specific question
- You're building understanding layer by layer
- You're citing tool outputs in your explanations
- You're running impact analysis before changes
- You're validating assumptions with cross-references

---

## Quick Reference: Tool Selection Decision Tree

```
Question: "How does X work?"
â”œâ”€ Need overview? â†’ enhanced_search
â”œâ”€ Need design patterns? â†’ semantic_intelligence
â””â”€ Need dependencies? â†’ graph_neighbors/traverse

Question: "Where should I make this change?"
â”œâ”€ Need to find code? â†’ enhanced_search
â”œâ”€ Need similar examples? â†’ vector_search
â””â”€ Need to assess impact? â†’ impact_analysis (REQUIRED)

Question: "Is this code good quality?"
â”œâ”€ Need anti-pattern detection? â†’ pattern_detection
â”œâ”€ Need design analysis? â†’ semantic_intelligence
â””â”€ Need performance data? â†’ performance_metrics

Question: "What depends on this?"
â”œâ”€ Need direct dependencies? â†’ graph_neighbors
â”œâ”€ Need full chain? â†’ graph_traverse
â””â”€ Need change impact? â†’ impact_analysis

Question: "Find similar code"
â”œâ”€ Semantic similarity? â†’ vector_search
â”œâ”€ Pattern matching? â†’ enhanced_search
â””â”€ Design patterns? â†’ semantic_intelligence
```

---

## Remember

**This is guidance, not law.** Adapt these patterns to your context. The gates exist to help you think clearly, not to constrain you.

**The core principles:**
1. **Think before you act** - Explain reasoning first
2. **Evidence over intuition** - Cite tool outputs
3. **Impact before change** - Always check blast radius
4. **Layer your understanding** - Build context incrementally

**When in doubt:** Start with `enhanced_search`, build understanding, then drill down with specific tools.

---

**Last Updated:** 2025-01-08
**Version:** 1.0.0
