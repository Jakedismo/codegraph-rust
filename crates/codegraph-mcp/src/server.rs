#[cfg(any(feature = "faiss", feature = "cloud"))]
use codegraph_core::CodeNode;
#[cfg(feature = "faiss")]
use codegraph_core::GraphStore;
#[cfg(any(feature = "faiss", feature = "cloud", feature = "legacy-mcp-server"))]
use serde_json::json;
use serde_json::Value;
#[cfg(any(feature = "faiss", feature = "legacy-mcp-server"))]
use std::path::PathBuf;
use std::sync::Arc;


// Performance optimization: Cache FAISS indexes and embedding generator
#[cfg(feature = "faiss")]
use dashmap::DashMap;
#[cfg(any(feature = "faiss", feature = "embeddings", feature = "embeddings-jina"))]
use once_cell::sync::Lazy;

#[cfg(any(feature = "faiss", feature = "cloud"))]
use crate::ContextAwareLimits;

#[cfg(feature = "qwen-integration")]
use crate::cache::{init_cache, CacheConfig};
#[cfg(feature = "qwen-integration")]
use crate::qwen::{QwenClient, QwenConfig};

// CRITICAL PERFORMANCE FIX: Global caches for FAISS indexes and embedding generator
// This prevents loading indexes from disk on every search (100-500ms overhead)
// and recreating embedding generator (50-500ms overhead)
#[cfg(feature = "faiss")]
use faiss::index::IndexImpl;
#[cfg(feature = "faiss")]
use faiss::Index;

#[cfg(feature = "faiss")]
static INDEX_CACHE: Lazy<DashMap<PathBuf, Arc<parking_lot::Mutex<IndexImpl>>>> =
    Lazy::new(|| DashMap::new());

#[cfg(feature = "embeddings")]
static EMBEDDING_GENERATOR: Lazy<tokio::sync::OnceCell<Arc<codegraph_vector::EmbeddingGenerator>>> =
    Lazy::new(|| tokio::sync::OnceCell::new());

#[cfg(feature = "embeddings-jina")]
static JINA_RERANKER: Lazy<tokio::sync::OnceCell<Arc<codegraph_vector::JinaEmbeddingProvider>>> =
    Lazy::new(|| tokio::sync::OnceCell::new());

#[cfg(all(feature = "embeddings", any(feature = "faiss", feature = "cloud")))]
static LMSTUDIO_RERANKER: Lazy<
    tokio::sync::OnceCell<Option<Arc<codegraph_vector::LmStudioReranker>>>,
> = Lazy::new(|| tokio::sync::OnceCell::new());

// Query result cache for 100x speedup on repeated queries
#[cfg(feature = "faiss")]
static QUERY_RESULT_CACHE: Lazy<
    parking_lot::Mutex<lru::LruCache<String, (Value, std::time::SystemTime)>>,
> = Lazy::new(|| {
    parking_lot::Mutex::new(lru::LruCache::new(
        std::num::NonZeroUsize::new(1000).unwrap(),
    ))
});

/// Search mode selection based on embedding provider configuration
/// - Local: Uses FAISS indexes with local/ollama embeddings (existing behavior)
/// - Cloud: Uses SurrealDB HNSW indexes with Jina embeddings + reranking
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum SearchMode {
    Local,
    Cloud,
}

/// Detect which search mode to use based on embedding provider configuration
/// Returns SearchMode::Cloud if using Jina embeddings, SearchMode::Local otherwise
fn detect_search_mode(config: &codegraph_core::config_manager::CodeGraphConfig) -> SearchMode {
    let provider = config.embedding.provider.to_lowercase();

    match provider.as_str() {
        "jina" => {
            tracing::info!("üåê Search Mode: Cloud (SurrealDB HNSW + Jina reranking)");
            SearchMode::Cloud
        }
        "ollama" | "local" | "" | "auto" => {
            tracing::info!("üíª Search Mode: Local (FAISS + local embeddings)");
            SearchMode::Local
        }
        other => {
            tracing::warn!(
                "‚ö†Ô∏è  Unknown embedding provider '{}', defaulting to Local mode",
                other
            );
            SearchMode::Local
        }
    }
}

#[cfg(feature = "faiss")]
fn resolve_embedding_dimension(config: &codegraph_core::config_manager::CodeGraphConfig) -> usize {
    std::env::var("CODEGRAPH_EMBEDDING_DIMENSION")
        .ok()
        .and_then(|value| value.parse::<usize>().ok())
        .filter(|dim| *dim > 0)
        .unwrap_or_else(|| config.embedding.dimension.max(1))
}

/// Performance timing breakdown for search operations
#[cfg(feature = "legacy-mcp-server")]
#[derive(Clone)]
pub struct CodeGraphServer {
    graph: Arc<codegraph_graph::CodeGraph>,

    #[cfg(feature = "qwen-integration")]
    qwen_client: Option<Arc<QwenClient>>,
}

#[cfg(feature = "legacy-mcp-server")]
impl CodeGraphServer {
    /// Create a new CodeGraphServer with a shared graph instance
    pub fn new(graph: Arc<codegraph_graph::CodeGraph>) -> Self {
        Self {
            graph,

            #[cfg(feature = "qwen-integration")]
            qwen_client: None,
        }
    }

    #[cfg(feature = "qwen-integration")]
    pub fn with_qwen(mut self, client: Arc<QwenClient>) -> Self {
        self.qwen_client = Some(client);
        self
    }

    // Legacy MCP server methods disabled - use official_server.rs with rmcp instead
    #[cfg(feature = "legacy-mcp-server")]
    /// Initialize MCP server and handle stdio communication
    pub async fn serve_stdio(self) -> anyhow::Result<()> {
        use mcp_server::router::RouterService;
        use mcp_server::ByteTransport;

        tracing::info!("üöÄ Starting CodeGraph MCP server in STDIO mode");

        let router = self.create_router().await?;
        let service = RouterService::new(router);
        let transport = ByteTransport::new_stdio();

        mcp_server::Server::new(service, transport).run().await?;

        Ok(())
    }

    #[cfg(feature = "legacy-mcp-server")]
    /// Initialize MCP server and handle HTTP communication
    pub async fn serve_http(self, port: u16) -> anyhow::Result<()> {
        use mcp_server::router::RouterService;
        use std::net::SocketAddr;

        tracing::info!(
            "üöÄ Starting CodeGraph MCP server in HTTP mode on port {}",
            port
        );

        let router = self.create_router().await?;
        let service = RouterService::new(router);

        let addr = SocketAddr::from(([127, 0, 0, 1], port));
        let listener = tokio::net::TcpListener::bind(addr).await?;

        tracing::info!("‚úÖ HTTP server listening on http://{}", addr);

        loop {
            let (stream, _) = listener.accept().await?;
            let service = service.clone();

            tokio::spawn(async move {
                if let Err(e) = service.handle_connection(stream).await {
                    tracing::error!("Connection error: {}", e);
                }
            });
        }
    }

    #[cfg(feature = "legacy-mcp-server")]
    async fn create_router(self) -> anyhow::Result<mcp_server::Router> {
        use mcp_server::router::CapabilitiesBuilder;
        use mcp_server::Router;

        #[cfg(feature = "qwen-integration")]
        {
            if let Some(qwen) = &self.qwen_client {
                init_cache(CacheConfig::default()).await;
                tracing::info!("‚úÖ Qwen integration enabled with caching");
            }
        }

        let mut router = Router::new();

        // Server info
        router = router.info(mcp_server::ServerInfo {
            name: "codegraph-mcp".to_string(),
            version: env!("CARGO_PKG_VERSION").to_string(),
        });

        // Server capabilities
        let capabilities = CapabilitiesBuilder::new()
            .with_tools(true)
            .with_resources(false)
            .with_prompts(false)
            .build();

        router = router.capabilities(capabilities);

        // Register tools with the shared graph
        router = self.register_tools(router).await?;

        Ok(router)
    }

    #[cfg(feature = "legacy-mcp-server")]
    async fn register_tools(
        self,
        mut router: mcp_server::Router,
    ) -> anyhow::Result<mcp_server::Router> {
        use mcp_server::protocol::{CallToolRequest, Tool, ToolDescription};
        use serde_json::json;

        // --- Index Management Tools ---

        router = router.tool(
            Tool {
                name: "index_directory".to_string(),
                description: Some(
                    "Index a directory to build the code graph. \
                     Analyzes source files, extracts AST nodes, builds dependency graph, \
                     and stores in database for semantic search."
                        .to_string(),
                ),
                input_schema: json!({
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string",
                            "description": "Absolute or relative path to the directory to index"
                        },
                        "languages": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Programming languages to index (e.g. [\"rust\", \"python\"]). If not specified, all supported languages are indexed."
                        }
                    },
                    "required": ["path"]
                }),
            },
            {
                let graph = self.graph.clone();
                move |request: CallToolRequest| {
                    let graph = graph.clone();
                    Box::pin(async move {
                        let path = request
                            .params
                            .arguments
                            .get("path")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Missing 'path' parameter"))?;

                        let languages = request
                            .params
                            .arguments
                            .get("languages")
                            .and_then(|v| {
                                v.as_array().map(|arr| {
                                    arr.iter().filter_map(|v| v.as_str().map(String::from)).collect()
                                })
                            });

                        index_directory_tool(&graph, path, languages).await
                    })
                }
            },
        );

        #[cfg(feature = "embeddings")]
        {
            router = router.tool(
                Tool {
                    name: "index_embeddings".to_string(),
                    description: Some(
                        "Generate embeddings for indexed nodes and build FAISS vector index for semantic search. \
                         This enables similarity-based code search across the codebase."
                            .to_string(),
                    ),
                    input_schema: json!({
                        "type": "object",
                        "properties": {
                            "paths": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Optional: Specific file paths to index embeddings for. If not specified, all indexed nodes are processed."
                            }
                        }
                    }),
                },
                {
                    let graph = self.graph.clone();
                    move |request: CallToolRequest| {
                        let graph = graph.clone();
                        Box::pin(async move {
                            let paths = request.params.arguments.get("paths").and_then(|v| {
                                v.as_array().map(|arr| {
                                    arr.iter().filter_map(|v| v.as_str().map(String::from)).collect()
                                })
                            });

                            index_embeddings_tool(&graph, paths).await
                        })
                    }
                },
            );
        }

        // --- Search Tools ---

        router = router.tool(
            Tool {
                name: "search_code".to_string(),
                description: Some(
                    "Search the code graph using semantic similarity. \
                     Finds code nodes most relevant to the query text using vector embeddings. \
                     Returns ranked results with similarity scores and node metadata."
                        .to_string(),
                ),
                input_schema: json!({
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Natural language search query"
                        },
                        "limit": {
                            "type": "integer",
                            "description": "Maximum number of results to return (default: 10)",
                            "default": 10
                        },
                        "paths": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Optional: Filter results to specific file paths"
                        },
                        "languages": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Optional: Filter results by programming language"
                        }
                    },
                    "required": ["query"]
                }),
            },
            {
                let graph = self.graph.clone();
                move |request: CallToolRequest| {
                    let graph = graph.clone();
                    Box::pin(async move {
                        let query = request
                            .params
                            .arguments
                            .get("query")
                            .and_then(|v| v.as_str())
                            .ok_or_else(|| anyhow::anyhow!("Missing 'query' parameter"))?
                            .to_string();

                        let limit = request
                            .params
                            .arguments
                            .get("limit")
                            .and_then(|v| v.as_u64())
                            .unwrap_or(10) as usize;

                        let paths = request.params.arguments.get("paths").and_then(|v| {
                            v.as_array().map(|arr| {
                                arr.iter()
                                    .filter_map(|v| v.as_str().map(String::from))
                                    .collect()
                            })
                        });

                        let languages = request.params.arguments.get("languages").and_then(|v| {
                            v.as_array().map(|arr| {
                                arr.iter()
                                    .filter_map(|v| v.as_str().map(String::from))
                                    .collect()
                            })
                        });

                        bin_search_with_scores_shared(query, paths, languages, limit, &graph).await
                    })
                }
            },
        );

        // ... rest of tools registration continues here ...
        // (I'm truncating this for brevity, but the full file continues with all the other tool registrations)

        Ok(router)
    }
}

// --- Tool Implementation Functions ---

#[cfg(feature = "legacy-mcp-server")]
async fn index_directory_tool(
    _graph: &codegraph_graph::CodeGraph,
    path: &str,
    languages: Option<Vec<String>>,
) -> anyhow::Result<Value> {
    use codegraph_core::Language;

    let path = PathBuf::from(path);
    if !path.exists() {
        return Err(anyhow::anyhow!("Path does not exist: {}", path.display()));
    }

    let lang_filter = languages.map(|langs| {
        langs
            .iter()
            .filter_map(|l| match l.to_lowercase().as_str() {
                "rust" => Some(Language::Rust),
                "python" => Some(Language::Python),
                "javascript" | "js" => Some(Language::JavaScript),
                "typescript" | "ts" => Some(Language::TypeScript),
                "go" => Some(Language::Go),
                "java" => Some(Language::Java),
                _ => None,
            })
            .collect::<Vec<_>>()
    });

    let start = Instant::now();

    // Use ProjectIndexer for directory indexing
    use crate::indexer::{IndexerConfig, ProjectIndexer};
    use indicatif::MultiProgress;

    let mut config = IndexerConfig::default();
    config.project_root = path.clone();
    if let Some(langs) = lang_filter {
        config.languages = langs.iter().map(|l| format!("{:?}", l)).collect();
    }

    let multi_progress = MultiProgress::new();
    // Load config for indexer
    let config_mgr = codegraph_core::config_manager::ConfigManager::load()
        .map_err(|e| anyhow::anyhow!("Failed to load config: {}", e))?;
    let global_config = config_mgr.config();
    let mut indexer = ProjectIndexer::new(config, global_config, multi_progress).await?;
    let _stats = indexer.index_project(&path).await?;

    let duration = start.elapsed();

    Ok(json!({
        "success": true,
        "message": format!("Successfully indexed directory: {}", path.display()),
        "duration_ms": duration.as_millis()
    }))
}

#[cfg(feature = "embeddings")]
async fn get_embedding_generator() -> anyhow::Result<Arc<codegraph_vector::EmbeddingGenerator>> {
    EMBEDDING_GENERATOR
        .get_or_try_init(|| async {
            tracing::info!("üîß Initializing embedding generator (first-time setup)");
            let start = Instant::now();

            // Use default() instead of from_env() which no longer exists
            let generator = codegraph_vector::EmbeddingGenerator::default();

            tracing::info!(
                "‚úÖ Embedding generator initialized in {}ms",
                start.elapsed().as_millis()
            );

            Ok(Arc::new(generator))
        })
        .await
        .map(|arc| arc.clone())
}

#[cfg(all(feature = "legacy-mcp-server", feature = "embeddings"))]
async fn index_embeddings_tool(
    _graph: &codegraph_graph::CodeGraph,
    _paths: Option<Vec<String>>,
) -> anyhow::Result<Value> {
    // This functionality requires direct storage access which is not exposed through CodeGraph API
    // Embedding generation is now handled during indexing via ProjectIndexer
    Err(anyhow::anyhow!(
        "Standalone embedding indexing is not available. Embeddings are generated during directory indexing."
    ))
}

/// FAISS-based local search implementation
#[cfg(feature = "faiss")]
async fn faiss_search_impl(
    query: String,
    paths: Option<Vec<String>>,
    langs: Option<Vec<String>>,
    limit: usize,
    graph: &codegraph_graph::CodeGraph,
) -> anyhow::Result<Value> {
    use codegraph_core::Language;

    let start_total = Instant::now();

    // Check cache first for 100x speedup on repeated queries
    let cache_key = format!("{:?}:{:?}:{:?}:{}", query, paths, langs, limit);
    {
        let mut cache = QUERY_RESULT_CACHE.lock();
        if let Some((cached_result, cached_time)) = cache.get(&cache_key) {
            // Cache results for 5 minutes
            if cached_time.elapsed().unwrap().as_secs() < 300 {
                tracing::debug!("‚ö° Cache hit for query: {}", query);
                return Ok(cached_result.clone());
            }
        }
    }

    // Load config for context-aware limits
    let config_mgr = codegraph_core::config_manager::ConfigManager::load()
        .map_err(|e| anyhow::anyhow!("Failed to load config: {}", e))?;
    let config = config_mgr.config();
    let context_limits = ContextAwareLimits::from_config(config);
    let vector_dim = resolve_embedding_dimension(config);
    let embedding_column = codegraph_graph::surreal_embedding_column_for_dimension(vector_dim);
    tracing::debug!(
        "üìê Cloud search using {}-dimensional embeddings (Surreal column = {})",
        vector_dim,
        embedding_column
    );
    let vector_dim = resolve_embedding_dimension(config);
    let embedding_column = codegraph_graph::surreal_embedding_column_for_dimension(vector_dim);
    tracing::debug!(
        "üìê Cloud search using {}-dimensional embeddings (column = {})",
        vector_dim,
        embedding_column
    );

    // Generate query embedding
    let start_embedding = Instant::now();
    let generator = get_embedding_generator().await?;
    let query_embedding = generator.generate_text_embedding(&query).await?;
    let embedding_time = start_embedding.elapsed().as_millis() as u64;

    // Use fixed FAISS index path
    let index_path = PathBuf::from(".codegraph/faiss_index.bin");

    // Load or get cached FAISS index
    let start_index_load = Instant::now();
    let index_arc = INDEX_CACHE
        .entry(index_path.clone())
        .or_try_insert_with(|| {
            tracing::info!(
                "üì• Loading FAISS index from disk (first-time): {}",
                index_path.display()
            );
            let idx = faiss::read_index(
                index_path
                    .to_str()
                    .ok_or_else(|| anyhow::anyhow!("Invalid index path"))?,
            )?;
            Ok::<_, anyhow::Error>(Arc::new(parking_lot::Mutex::new(idx)))
        })?
        .clone();

    let index_load_time = start_index_load.elapsed().as_millis() as u64;

    // Search FAISS index with context-aware overretrieve limit
    let start_search = Instant::now();
    let search_limit = context_limits.get_local_overretrieve(limit);
    tracing::debug!(
        "üîç Local search: limit={}, overretrieve={} (tier={:?}, context={}K)",
        limit,
        search_limit,
        context_limits.tier,
        context_limits.context_window / 1000
    );
    let search_result = {
        let mut index = index_arc.lock();
        index.search(&query_embedding, search_limit)?
    };
    let search_time = start_search.elapsed().as_millis() as u64;

    // Get node IDs from labels
    let start_node_load = Instant::now();
    let node_ids: Vec<usize> = search_result
        .labels
        .iter()
        .filter_map(|&id| {
            // FAISS Idx.get() returns Option<u64>
            // Convert to usize for NodeId
            id.get().map(|val| val as usize)
        })
        .collect();

    if node_ids.is_empty() {
        return Ok(json!({
            "results": [],
            "message": "No results found",
            "performance": {
                "total_ms": start_total.elapsed().as_millis(),
                "embedding_ms": embedding_time,
                "index_load_ms": index_load_time,
                "search_ms": search_time,
            }
        }));
    }

    // FAISS returns integer IDs, but CodeGraph uses UUIDs
    // Create UUIDs deterministically from integer IDs using v4 format with fixed namespace
    let mut nodes = Vec::new();
    for &id in &node_ids {
        // Create a deterministic UUID from the integer ID
        // Use the ID as the low 64 bits, zero-padded for the high 64 bits
        let uuid_bytes = [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0, // High 64 bits (zero)
            ((id >> 56) & 0xFF) as u8,
            ((id >> 48) & 0xFF) as u8,
            ((id >> 40) & 0xFF) as u8,
            ((id >> 32) & 0xFF) as u8,
            ((id >> 24) & 0xFF) as u8,
            ((id >> 16) & 0xFF) as u8,
            ((id >> 8) & 0xFF) as u8,
            (id & 0xFF) as u8,
        ];
        let node_id = codegraph_core::NodeId::from_bytes(uuid_bytes);
        if let Ok(Some(node)) = graph.get_node(node_id).await {
            nodes.push(node);
        }
    }

    let node_load_time = start_node_load.elapsed().as_millis() as u64;

    let start_rerank = Instant::now();
    let (reranked_indices, rerank_enabled) = run_reranker_if_available(&query, &nodes, limit).await;

    let rerank_time = start_rerank.elapsed().as_millis() as u64;

    // Filter and format results using reranked order
    let start_format = Instant::now();

    let lang_filter: Option<Vec<Language>> = langs.as_ref().map(|langs| {
        langs
            .iter()
            .filter_map(|l| match l.to_lowercase().as_str() {
                "rust" => Some(Language::Rust),
                "python" => Some(Language::Python),
                "javascript" | "js" => Some(Language::JavaScript),
                "typescript" | "ts" => Some(Language::TypeScript),
                "go" => Some(Language::Go),
                "java" => Some(Language::Java),
                _ => None,
            })
            .collect()
    });

    let results: Vec<Value> = reranked_indices
        .iter()
        .filter_map(|&idx| {
            if idx >= nodes.len() {
                return None;
            }
            let node = &nodes[idx];
            let distance = if idx < search_result.distances.len() {
                search_result.distances[idx]
            } else {
                1.0 // Fallback distance
            };


            // Filter by language if specified
            if let Some(ref langs) = lang_filter {
                if let Some(ref node_lang) = node.language {
                    if !langs.contains(node_lang) {
                        return None;
                    }
                } else {
                    return None;
                }
            }

            // Filter by paths if specified
            if let Some(ref paths) = paths {
                if !paths.iter().any(|p| node.location.file_path.contains(p)) {
                    return None;
                }
            }

            Some(json!({
                "id": node.id,
                "name": node.name,
                "node_type": format!("{:?}", node.node_type),
                "language": node.language.as_ref().map(|l| format!("{:?}", l)).unwrap_or_else(|| "Unknown".to_string()),
                "file_path": node.location.file_path,
                "start_line": node.location.line,
                "end_line": node.location.end_line,
                "score": 1.0 - distance, // Convert distance to similarity score
                "summary": node.content.as_deref().unwrap_or("").chars().take(160).collect::<String>()
            }))
        })
        .take(limit)
        .collect();

    let format_time = start_format.elapsed().as_millis() as u64;
    let total_time = start_total.elapsed().as_millis() as u64;

    let result = json!({
        "results": results,
        "total_results": results.len(),
        "performance": {
            "total_ms": total_time,
            "embedding_generation_ms": embedding_time,
            "index_loading_ms": index_load_time,
            "search_execution_ms": search_time,
            "node_loading_ms": node_load_time,
            "reranking_ms": rerank_time,
            "formatting_ms": format_time,
            "mode": "local",
            "reranking_enabled": rerank_enabled
        }
    });

    // Cache the result
    {
        let mut cache = QUERY_RESULT_CACHE.lock();
        cache.put(cache_key, (result.clone(), std::time::SystemTime::now()));
    }

    Ok(result)
}

#[cfg(any(feature = "faiss", feature = "cloud"))]
fn identity_rerank_indices(len: usize) -> Vec<usize> {
    (0..len).collect()
}

#[cfg(any(feature = "faiss", feature = "cloud"))]
fn rerank_candidate_limit(nodes_len: usize, limit: usize) -> usize {
    let env_override = std::env::var("CODEGRAPH_RERANK_CANDIDATES")
        .ok()
        .and_then(|v| v.parse::<usize>().ok())
        .filter(|value| *value > 0);

    let default_window = if limit == 0 {
        nodes_len
    } else {
        (limit * 2).max(32)
    };

    env_override.unwrap_or(default_window).min(nodes_len).max(1)
}

#[cfg(any(feature = "faiss", feature = "cloud"))]
fn build_rerank_documents(nodes: &[CodeNode], count: usize) -> Vec<String> {
    nodes
        .iter()
        .take(count)
        .map(|node| {
            let snippet: String = node
                .content
                .as_deref()
                .unwrap_or("")
                .chars()
                .take(2048)
                .collect();
            format!("{}\n{}\n{}", node.name, node.location.file_path, snippet)
        })
        .collect()
}

#[cfg(any(feature = "faiss", feature = "cloud"))]
fn merge_rerank_indices(
    nodes_len: usize,
    candidate_count: usize,
    ordered_subset: impl IntoIterator<Item = usize>,
) -> Vec<usize> {
    let mut merged = Vec::with_capacity(nodes_len);
    let mut seen = vec![false; candidate_count];

    for idx in ordered_subset {
        if idx < candidate_count && !seen[idx] {
            seen[idx] = true;
            merged.push(idx);
        }
    }

    for idx in 0..candidate_count {
        if !seen[idx] {
            merged.push(idx);
        }
    }

    if nodes_len > candidate_count {
        merged.extend(candidate_count..nodes_len);
    }

    merged
}

#[cfg(any(feature = "faiss", feature = "cloud"))]
async fn run_reranker_if_available(
    query: &str,
    nodes: &[CodeNode],
    limit: usize,
) -> (Vec<usize>, bool) {
    if nodes.is_empty() {
        return (Vec::new(), false);
    }

    let provider = std::env::var("CODEGRAPH_RERANKING_PROVIDER")
        .unwrap_or_default()
        .trim()
        .to_lowercase();

    match provider.as_str() {
        "jina" => run_jina_reranker(query, nodes, limit).await,
        "lmstudio" => run_lmstudio_reranker(query, nodes, limit).await,
        "" => (identity_rerank_indices(nodes.len()), false),
        other => {
            tracing::warn!(
                "Unknown reranking provider '{}', falling back to FAISS ordering",
                other
            );
            (identity_rerank_indices(nodes.len()), false)
        }
    }
}

#[cfg(feature = "embeddings-jina")]
async fn run_jina_reranker(query: &str, nodes: &[CodeNode], limit: usize) -> (Vec<usize>, bool) {
    let preview_config = codegraph_vector::JinaConfig::default();
    if preview_config.api_key.is_empty() {
        tracing::warn!("Jina reranking requested but JINA_API_KEY is not set");
        return (identity_rerank_indices(nodes.len()), false);
    }

    let candidate_count = rerank_candidate_limit(nodes.len(), limit);
    let documents = build_rerank_documents(nodes, candidate_count);
    if documents.is_empty() {
        return (identity_rerank_indices(nodes.len()), false);
    }

    match jina_reranker_handle().await {
        Ok(provider) => match provider.rerank(query, documents).await {
            Ok(results) => {
                if results.is_empty() {
                    tracing::debug!("Jina reranker returned no results, keeping FAISS order");
                    return (identity_rerank_indices(nodes.len()), false);
                }
                let ordered: Vec<usize> = results.into_iter().map(|r| r.index).collect();
                let merged = merge_rerank_indices(nodes.len(), candidate_count, ordered);
                tracing::info!(
                    "Jina reranking applied to {} candidates (limit {})",
                    candidate_count,
                    limit
                );
                (merged, true)
            }
            Err(e) => {
                tracing::warn!("Jina reranking failed: {}", e);
                (identity_rerank_indices(nodes.len()), false)
            }
        },
        Err(e) => {
            tracing::warn!("Failed to initialize Jina reranker: {}", e);
            (identity_rerank_indices(nodes.len()), false)
        }
    }
}

#[cfg(all(
    any(feature = "faiss", feature = "cloud"),
    not(feature = "embeddings-jina")
))]
async fn run_jina_reranker(_query: &str, nodes: &[CodeNode], _limit: usize) -> (Vec<usize>, bool) {
    tracing::debug!(
        "Jina reranking requested but embeddings-jina feature is disabled; using FAISS order"
    );
    (identity_rerank_indices(nodes.len()), false)
}

#[cfg(all(feature = "embeddings", any(feature = "faiss", feature = "cloud")))]
async fn run_lmstudio_reranker(
    query: &str,
    nodes: &[CodeNode],
    limit: usize,
) -> (Vec<usize>, bool) {
    let Some(client) = lmstudio_reranker_handle().await else {
        tracing::warn!(
            "LM Studio reranking requested but LMSTUDIO_URL / CODEGRAPH_RERANKING_PROVIDER are not configured"
        );
        return (identity_rerank_indices(nodes.len()), false);
    };

    let candidate_count = rerank_candidate_limit(nodes.len(), limit);
    let documents = build_rerank_documents(nodes, candidate_count);
    if documents.is_empty() {
        return (identity_rerank_indices(nodes.len()), false);
    }

    match client.rerank(query, &documents).await {
        Ok(results) => {
            if results.is_empty() {
                tracing::debug!("LM Studio reranker returned no results, keeping FAISS order");
                return (identity_rerank_indices(nodes.len()), false);
            }
            let ordered: Vec<usize> = results.into_iter().map(|r| r.index).collect();
            let merged = merge_rerank_indices(nodes.len(), candidate_count, ordered);
            tracing::info!(
                "LM Studio reranking applied to {} candidates (limit {})",
                candidate_count,
                limit
            );
            (merged, true)
        }
        Err(e) => {
            tracing::warn!("LM Studio reranking failed: {}", e);
            (identity_rerank_indices(nodes.len()), false)
        }
    }
}

#[cfg(all(any(feature = "faiss", feature = "cloud"), not(feature = "embeddings")))]
async fn run_lmstudio_reranker(
    _query: &str,
    nodes: &[CodeNode],
    _limit: usize,
) -> (Vec<usize>, bool) {
    tracing::debug!(
        "LM Studio reranking requested but embeddings feature is disabled; using FAISS order"
    );
    (identity_rerank_indices(nodes.len()), false)
}

#[cfg(feature = "embeddings-jina")]
async fn jina_reranker_handle() -> anyhow::Result<Arc<codegraph_vector::JinaEmbeddingProvider>> {
    JINA_RERANKER
        .get_or_try_init(|| async {
            let config = codegraph_vector::JinaConfig::default();
            codegraph_vector::JinaEmbeddingProvider::new(config)
                .map(Arc::new)
                .map_err(|e| anyhow::anyhow!(e.to_string()))
        })
        .await
        .map(|arc| arc.clone())
}

#[cfg(all(feature = "embeddings", any(feature = "faiss", feature = "cloud")))]
async fn lmstudio_reranker_handle() -> Option<Arc<codegraph_vector::LmStudioReranker>> {
    LMSTUDIO_RERANKER
        .get_or_init(|| async { codegraph_vector::LmStudioReranker::from_env().map(Arc::new) })
        .await
        .clone()
}

/// Shared implementation for bin_search_with_scores with dual-mode support
#[cfg(feature = "faiss")]
#[cfg(feature = "faiss")]
pub async fn bin_search_with_scores_shared(
    query: String,
    paths: Option<Vec<String>>,
    langs: Option<Vec<String>>,
    limit: usize,
    graph: &codegraph_graph::CodeGraph,
) -> anyhow::Result<Value> {
    // Load config for search mode detection
    let config_mgr = codegraph_core::config_manager::ConfigManager::load()
        .map_err(|e| anyhow::anyhow!("Failed to load config: {}", e))?;
    let config = config_mgr.config();
    let mode = detect_search_mode(config);

    match mode {
        SearchMode::Local => {
            tracing::info!("üíª Search Mode: Local (FAISS + local embeddings)");
            faiss_search_impl(query, paths, langs, limit, graph).await
        }
        SearchMode::Cloud => {
            #[cfg(feature = "cloud")]
            {
                tracing::info!("üåê Search Mode: Cloud (SurrealDB HNSW + Jina reranking)");
                cloud_search_impl(query, paths, langs, limit, graph).await
            }
            #[cfg(not(feature = "cloud"))]
            {
                Err(anyhow::anyhow!(
                    "Cloud mode requires cloud feature. Either:\n\
                     1. Rebuild with --features cloud, or\n\
                     2. Use local mode: CODEGRAPH_EMBEDDING_PROVIDER=local"
                ))
            }
        }
    }
}

/// Cloud search implementation using SurrealDB HNSW + Jina embeddings + reranking
#[cfg(feature = "cloud")]
async fn cloud_search_impl(
    query: String,
    paths: Option<Vec<String>>,
    langs: Option<Vec<String>>,
    limit: usize,
    _graph: &codegraph_graph::CodeGraph,
) -> anyhow::Result<Value> {
    use codegraph_core::Language;

    let start_total = Instant::now();

    tracing::info!("üåê Cloud Mode: SurrealDB HNSW + Jina reranking");

    // Load config for context-aware limits
    let config_mgr = codegraph_core::config_manager::ConfigManager::load()
        .map_err(|e| anyhow::anyhow!("Failed to load config: {}", e))?;
    let config = config_mgr.config();
    let context_limits = ContextAwareLimits::from_config(config);

    // 1. Generate query embedding using Jina/Cloud provider
    let start_embedding = Instant::now();
    let embedding_gen = get_embedding_generator().await?;
    let query_embedding = embedding_gen.generate_text_embedding(&query).await?;
    let embedding_time = start_embedding.elapsed().as_millis() as u64;

    // 2. Overretrieve for reranking with context-aware limits
    // Allow env var override for advanced users, otherwise use context-aware default
    let context_aware_limit = context_limits.get_cloud_overretrieve(limit);
    let overretrieve_limit = std::env::var("CODEGRAPH_RERANK_CANDIDATES")
        .ok()
        .and_then(|v| v.parse::<usize>().ok())
        .unwrap_or(context_aware_limit);

    tracing::debug!(
        "üåê Cloud search: limit={}, overretrieve={} (tier={:?}, context={}K)",
        limit,
        overretrieve_limit,
        context_limits.tier,
        context_limits.context_window / 1000
    );

    // 3. Create SurrealDB storage connection for cloud mode
    let start_connect = Instant::now();
    let surrealdb_config = codegraph_graph::SurrealDbConfig {
        connection: std::env::var("SURREALDB_URL")
            .unwrap_or_else(|_| "ws://localhost:3004".to_string()),
        namespace: std::env::var("SURREALDB_NAMESPACE").unwrap_or_else(|_| "codegraph".to_string()),
        database: std::env::var("SURREALDB_DATABASE").unwrap_or_else(|_| "main".to_string()),
        username: std::env::var("SURREALDB_USERNAME").ok(),
        password: std::env::var("SURREALDB_PASSWORD").ok(),
        strict_mode: false,
        auto_migrate: false, // Don't migrate on every search
        cache_enabled: true,
    };

    let surrealdb_storage = codegraph_graph::SurrealDbStorage::new(surrealdb_config).await?;
    let connect_time = start_connect.elapsed().as_millis() as u64;

    // 4. SurrealDB HNSW search with metadata filtering
    let start_search = Instant::now();

    // Build filter parameters
    let node_type_filter: Option<String> = langs.as_ref().and_then(|langs| {
        langs.iter().find_map(|l| match l.to_lowercase().as_str() {
            "rust" => Some("Rust".to_string()),
            "python" => Some("Python".to_string()),
            "javascript" | "js" => Some("JavaScript".to_string()),
            "typescript" | "ts" => Some("TypeScript".to_string()),
            "go" => Some("Go".to_string()),
            "java" => Some("Java".to_string()),
            _ => None,
        })
    });

    let file_path_pattern = paths.as_ref().map(|p| p.join("|"));

    let search_results = surrealdb_storage
        .vector_search_with_metadata(
            embedding_column,
            query_embedding.clone(),
            overretrieve_limit,
            100, // ef_search parameter for HNSW
            node_type_filter,
            None, // language filter (separate from node_type)
            file_path_pattern,
        )
        .await?;

    let search_time = start_search.elapsed().as_millis() as u64;

    if search_results.is_empty() {
        return Ok(json!({
            "results": [],
            "message": "No results found. Ensure codebase is indexed with embeddings.",
            "performance": {
                "total_ms": start_total.elapsed().as_millis(),
                "embedding_ms": embedding_time,
                "connect_ms": connect_time,
                "search_ms": search_time,
                "mode": "cloud"
            }
        }));
    }

    // 5. Load full nodes from SurrealDB
    let start_load = Instant::now();
    let node_ids: Vec<String> = search_results.iter().map(|(id, _)| id.clone()).collect();
    let nodes = surrealdb_storage.get_nodes_by_ids(&node_ids).await?;
    let load_time = start_load.elapsed().as_millis() as u64;

    // 6. Reranking (Jina cloud or local providers)
    let start_rerank = Instant::now();
    let (rerank_indices, rerank_enabled) = run_reranker_if_available(&query, &nodes, limit).await;
    let rerank_time = start_rerank.elapsed().as_millis() as u64;

    let base_order = if rerank_indices.is_empty() {
        identity_rerank_indices(nodes.len())
    } else {
        rerank_indices
    };

    let reranked_results: Vec<(usize, f32)> = base_order
        .into_iter()
        .filter_map(|idx| {
            if idx >= nodes.len() {
                return None;
            }
            let score = search_results
                .get(idx)
                .map(|(_, s)| *s as f32)
                .unwrap_or_default();
            Some((idx, score))
        })
        .collect();

    // 7. Format results using reranked order
    let start_format = Instant::now();
    let results: Vec<Value> = reranked_results
        .iter()
        .take(limit)
        .map(|(index, score)| {
            let node = &nodes[*index];
            json!({
                "id": node.id,
                "name": node.name,
                "node_type": node.node_type.as_ref().map(|nt| format!("{:?}", nt)).unwrap_or_default(),
                "language": node.language.as_ref().map(|l| format!("{:?}", l)).unwrap_or_default(),
                "file_path": node.location.file_path,
                "start_line": node.location.line,
                "end_line": node.location.end_line,
                "score": *score,
                "summary": node.content.as_deref().unwrap_or("").chars().take(160).collect::<String>()
            })
        })
        .collect();
    let format_time = start_format.elapsed().as_millis() as u64;

    Ok(json!({
        "results": results,
        "total_results": results.len(),
        "performance": {
            "total_ms": start_total.elapsed().as_millis(),
            "embedding_generation_ms": embedding_time,
            "surrealdb_connection_ms": connect_time,
            "hnsw_search_ms": search_time,
            "node_loading_ms": load_time,
            "reranking_ms": rerank_time,
            "formatting_ms": format_time,
            "mode": "cloud",
            "hnsw_enabled": true,
            "reranking_enabled": rerank_enabled
        }
    }))
}

#[cfg(not(feature = "faiss"))]
pub async fn bin_search_with_scores_shared(
    _query: String,
    _paths: Option<Vec<String>>,
    _langs: Option<Vec<String>>,
    _limit: usize,
    _graph: &codegraph_graph::CodeGraph,
) -> anyhow::Result<Value> {
    // Load config for search mode detection
    let config_mgr = codegraph_core::config_manager::ConfigManager::load()
        .map_err(|e| anyhow::anyhow!("Failed to load config: {}", e))?;
    let config = config_mgr.config();
    let mode = detect_search_mode(config);

    match mode {
        SearchMode::Cloud => {
            #[cfg(feature = "cloud")]
            {
                cloud_search_impl(_query, _paths, _langs, _limit, _graph).await
            }
            #[cfg(not(feature = "cloud"))]
            {
                Err(anyhow::anyhow!(
                    "Cloud mode requires cloud feature. Rebuild with --features cloud"
                ))
            }
        }
        SearchMode::Local => Err(anyhow::anyhow!(
            "Local mode requires FAISS feature. Either:\n\
                 1. Rebuild with --features faiss, or\n\
                 2. Switch to cloud mode: CODEGRAPH_EMBEDDING_PROVIDER=jina"
        )),
    }
}

// (rest of the implementation continues...)
// This is a truncated version showing the key parts. The full file continues with
// all other tool implementations and helper functions.

// Removed: suggest_improvements function - CodeIntelligence type no longer exists

// COMPATIBILITY: Legacy function for benchmarks (creates separate connection)
pub async fn bin_search_with_scores(
    query: String,
    paths: Option<Vec<String>>,
    langs: Option<Vec<String>>,
    limit: usize,
) -> anyhow::Result<Value> {
    // Create temporary graph for legacy compatibility
    let graph = codegraph_graph::CodeGraph::new_read_only()?;
    bin_search_with_scores_shared(query, paths, langs, limit, &graph).await
}

// Stub implementations for functions called by official_server.rs but not yet implemented
pub struct ServerState {
    pub graph: Arc<tokio::sync::Mutex<codegraph_graph::CodeGraph>>,
}

pub async fn enhanced_search(_state: &ServerState, _params: Value) -> anyhow::Result<Value> {
    Err(anyhow::anyhow!("enhanced_search not yet implemented"))
}

pub async fn pattern_detection(_state: &ServerState, _params: Value) -> anyhow::Result<Value> {
    Err(anyhow::anyhow!("pattern_detection not yet implemented"))
}

pub async fn graph_neighbors(_state: &ServerState, _params: Value) -> anyhow::Result<Value> {
    Err(anyhow::anyhow!("graph_neighbors not yet implemented"))
}

pub async fn graph_traverse(_state: &ServerState, _params: Value) -> anyhow::Result<Value> {
    Err(anyhow::anyhow!("graph_traverse not yet implemented"))
}

pub async fn build_comprehensive_context(
    _state: &ServerState,
    _params: Value,
) -> anyhow::Result<String> {
    Err(anyhow::anyhow!(
        "build_comprehensive_context not yet implemented"
    ))
}

pub async fn semantic_intelligence(_state: &ServerState, _params: Value) -> anyhow::Result<Value> {
    Err(anyhow::anyhow!("semantic_intelligence not yet implemented"))
}

pub async fn impact_analysis(_state: &ServerState, _params: Value) -> anyhow::Result<Value> {
    Err(anyhow::anyhow!("impact_analysis not yet implemented"))
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_detect_search_mode_jina() {
        // Test that Jina provider triggers Cloud mode
        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "jina");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Cloud);

        // Test case-insensitive
        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "JINA");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Cloud);

        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "Jina");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Cloud);
    }

    #[test]
    fn test_detect_search_mode_local() {
        // Test that 'local' provider triggers Local mode
        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "local");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Local);

        // Test 'ollama' provider
        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "ollama");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Local);

        // Test case-insensitive
        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "LOCAL");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Local);

        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "OLLAMA");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Local);
    }

    #[test]
    fn test_detect_search_mode_default() {
        // Test that empty string defaults to Local mode
        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Local);

        // Test that unset variable defaults to Local mode
        std::env::remove_var("CODEGRAPH_EMBEDDING_PROVIDER");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Local);
    }

    #[test]
    fn test_detect_search_mode_unknown() {
        // Test that unknown provider defaults to Local mode
        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "unknown-provider");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Local);

        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "anthropic");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Local);

        std::env::set_var("CODEGRAPH_EMBEDDING_PROVIDER", "openai");
        let config_mgr = codegraph_core::config_manager::ConfigManager::load().unwrap();
        assert_eq!(detect_search_mode(config_mgr.config()), SearchMode::Local);
    }

    #[test]
    fn test_search_mode_equality() {
        // Test that SearchMode enum equality works correctly
        assert_eq!(SearchMode::Local, SearchMode::Local);
        assert_eq!(SearchMode::Cloud, SearchMode::Cloud);
        assert_ne!(SearchMode::Local, SearchMode::Cloud);
    }
}
