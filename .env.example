# CodeGraph Configuration Example
# Copy this file to .env and update the values for your environment

 ============================================================================
# Storage and Installation Configuration (LEGACY FOR FAISS+RocksDB)
# ============================================================================

# Storage path for CodeGraph data (indexes, cache, etc.)
# CODEGRAPH_STORAGE=/path/to/storage
# CODEGRAPH_STORAGE_PATH=/path/to/storage  # Alternative name

# Installation directory for binaries
# CODEGRAPH_INSTALL_DIR=$HOME/.local/bin

# Project identification (for multi-project setups)
# CODEGRAPH_PROJECT_ID=my-project
# CODEGRAPH_ORGANIZATION_ID=my-org
# CODEGRAPH_REPOSITORY_URL=https://github.com/user/repo
# CODEGRAPH_DOMAIN=example.com

# ============================================================================
# Embedding Provider Configuration (Local)
# ============================================================================
# Provider options: "onnx", "ollama", "openai", "jina", or "lmstudio"
# CODEGRAPH_EMBEDDING_PROVIDER=ollama

# ONNX: Specify model path (or leave empty for auto-detection from HuggingFace cache)
# CODEGRAPH_LOCAL_MODEL=/path/to/your/onnx/model

# Ollama: Specify embedding model name
# CODEGRAPH_EMBEDDING_MODEL=all-minilm:latest
# CODEGRAPH_OLLAMA_URL=http://localhost:11434
# CODEGRAPH_EMBEDDING_MODEL=qwen3-embedding:4b
# CODEGRAPH_EMBEDDING_DIMENSION=2560 # Important! Has to match the actual output dimensionality of the embedding model you use!
# CODEGRAPH_MAX_CHUNK_TOKENS=28000 # Important! The ctx window of you embedding model qwen3-embedding:4b has 32K tokens so we chunk at 28K and f.ex. all-minilm has 512 tokens so chunk at f.ex. 256 tokens
# Semantic-Chunker is used by default by Codegraph

# LM Studio: Best for MLX + Flash Attention 2 (recommended on macOS)
# Default: jina-embeddings-v4 (2048 or 1024 dimensions)
# CODEGRAPH_EMBEDDING_PROVIDER=lmstudio
# CODEGRAPH_EMBEDDING_MODEL=jinaai/jina-embeddings-v4
# CODEGRAPH_LMSTUDIO_URL=http://localhost:1234
# CODEGRAPH_EMBEDDING_DIMENSION=2048

# ============================================================================
# Reranking Provider Configuration (Local)
# ============================================================================
# CODEGRAPH_RERANKING_PROVIDER = lmstudio
# CODEGRAPH_RERANKING_MODEL = jina-reranker-v3
# CODEGRAPH_RERANKING_CANDIDATES = 512 # (fetched results by semantic search: semantic search > 512 results > rerank > top 10)


# ============================================================================
# Embedding Provider Configuration (Cloud)
# ============================================================================
# OpenAI: Model name (API key configured below in Security section)
# CODEGRAPH_EMBEDDING_MODEL=text-embedding-3-small
# CODEGRAPH_EMBEDDING_DIMENSION=1536 # Important! Has to match the actual output dimensionality of the embedding model you use! 1536 for small, 3072 for large

# Batch size for embedding generation (applies to all providers)
# CODEGRAPH_EMBEDDING_BATCH_SIZE=32  # Default: 32, valid range: 1-64 Ollama and LMStudio throttle the throughput with larger batches even when you would have the memory to do so the gains of f.ex. 1024 batches are not that significant to compared to 64

# Jina AI: Cloud embeddings with reranking (requires JINA_API_KEY)
# CODEGRAPH_EMBEDDING_PROVIDER=jina
# JINA_EMBEDDING_MODEL=jina-embeddings-v4
# JINA_EMBEDDINGS_MODEL=jina-embeddings-v4  # Alternative name
# JINA_EMBEDDING_DIMENSION=2048 (supports 1024, 512, 256 but only 1024 has pre-defined column and HNSW index in surrealdb schema)
# JINA_API_KEY=your-jina-api-key-here
# JINA_MAX_TEXTS=512 # Leverage Jina API Batch functionality max 512 documents with 8192 tokens each remember to set --max-concurrent 1 when indexing
# JINA_MAX_TOKENS=7000 # Substitute for CODEGRAPH_MAX_TOKENS
# JINA_API_BASE=https://api.jina.ai/v1
# JINA_API_TASK=code.passage # used when embedding data, code.query is used when searching data
# JINA_TASK=code.passage  # Alternative name for JINA_API_TASK
# JINA_LATE_CHUNKING=false # Leverage Jina AIs advanced long-context chunking features for more accurate embeddings
# JINA_TRUNCATE=true # truncate texts and embeddings if over dimension limit
# JINA_REQUEST_DELAY_MS=600 # small delay not to throttle the API when batching

# Jina Reranking Configuration
# CODEGRAPH_RERANKING_PROVIDER=jina
# CODEGRAPH_RERANK_CANDIDATES=512 # Leverage Jina API Batch functionality also here
# JINA_ENABLE_RERANKING=true
# JINA_RERANKING_ENABLED=true  # Alternative name
# JINA_RERANKING_MODEL=jina-reranker-v3
# JINA_RERANKING_TOP_N=10 # (fetched results by semantic search: semantic search > 512 results > rerank > top 10)
# CODEGRAPH_RERANKING_CANDIDATES = 512 # (fetched results by semantic search: semantic search > 512 results > rerank > top 10)

# Jina Relationship Embeddings Configuration, You don't really need to touch this CODEGRAPH_EMBEDDING_BATCH_SIZE is usually enough
# JINA_REL_BATCH_SIZE=50
# JINA_REL_MAX_TEXTS=50

# Jina Batching for Large Indexing Operations
# CODEGRAPH_JINA_BATCH_SIZE=2000
# CODEGRAPH_JINA_BATCH_MINUTES=9.0 # used by codegraph estimate command, this is how long it took on average to index the codegraph codebase with Jina

# ============================================================================
# Dual-Mode Search Configuration
# ============================================================================
# CodeGraph supports two search modes based on CODEGRAPH_EMBEDDING_PROVIDER:
#
# Local Mode (FAISS + local/ollama embeddings)
# ---------------------------------------------
# - Uses FAISS for in-memory vector search
# - Embeddings: ONNX, Ollama, or LM Studio
# - Best for: Desktop development, privacy-focused setups
# - Requires: Build with --features faiss
# Example:
#   CODEGRAPH_EMBEDDING_PROVIDER=local  # or ollama or lmstudio
#
# Cloud Mode (SurrealDB HNSW + Jina embeddings + reranking)
# ----------------------------------------------------------
# - Uses SurrealDB HNSW indexes for scalable vector search
# - Embeddings: Jina AI (Variable Matryosha dimensions - check what the model outputs and adjust schema/codegraph.surql HNSW vector dims)
# - Supported Jina AI embedding model is f.ex. jina-embeddings-v4
# - Reranking: Jina reranker-v3 for improved relevance
# - Best for: Cloud deployments, multi-user systems, scalability
# - Requires: SurrealDB instance, Jina API key
# Example:
#   CODEGRAPH_EMBEDDING_PROVIDER=jina
#   JINA_API_KEY=your-jina-api-key-here

# Cloud features toggle
# CODEGRAPH_CLOUD_ENABLED=false

# ============================================================================
# SurrealDB Configuration (required for cloud mode)
# ============================================================================
# SurrealDB connection (local or cloud)
# CODEGRAPH_SURREALDB_URL=ws://localhost:3004
# SURREALDB_URL=ws://localhost:3004  # Alternative name
# CODEGRAPH_SURREALDB_NAMESPACE=codegraph
# SURREALDB_NAMESPACE=codegraph  # Alternative name
# CODEGRAPH_SURREALDB_DATABASE=main
# SURREALDB_DATABASE=main  # Alternative name
# CODEGRAPH_SURREALDB_USERNAME=root
# SURREALDB_USERNAME=root  # Alternative name
# CODEGRAPH_SURREALDB_PASSWORD=root
# SURREALDB_PASSWORD=root  # Alternative name
#
# Important: HNSW index dimension must match embedding provider
# - Jina: Variable Matryoska dimensions depending on model 2048-dims configured
# - OpenAI: Small 1536 dimensions, Large 3072 dimensions
# - Local ONNX: typically 384 qdrant/all-mini-llm-onnx
# - Local Ollama: qwen3-embedder:0.6b-8b 1024, 2056, 4096, embeddingsgemma: 768, all-minilm: 384
# - 384 (all-minilm:latest)
# - 768 (embeddingsgemma:latest)
# - 1024 (qwen3-embedding:0.6b)
# - 1536 (text-embedding-3-small)
# - 2048 (jina-embeddings-v4)
# - 2056 (qwen3-embedding:4b)
# - 3072 (text-embedding-3-large)
# - 4096 (qwen3-embedding:8b)
# For pure speed use onnx or ollama all-mini-llm ~10mins to index the whole codegraph codebase
# For considerably better retriaval switch to qwen3-embedding:0.6b or code language aware 4b or 8b versions for even better accuracy
# Scale qwen3-embedding model per criticality of accuracy on Ollma
# For enhancing local accuracy enable CODEGRAPH_RERANKING_PROVIDER=lmstudio and f.ex. CODEGRAPH_RERANKING_MODEL=qwen-reranker-3:0.6b
# For true SOTA use jina provider and jina-embeddings-v4 with the jina-reranker-v3 - takes longer to index but works better

# Hierarchical config system also supports CODEGRAPH__* prefix for nested config
# Example: CODEGRAPH__DATABASE__BACKEND=surrealdb
# Example: CODEGRAPH__DATABASE__SURREALDB__CONNECTION=ws://localhost:8000
# Example: CODEGRAPH__DATABASE__SURREALDB__PASSWORD=your_password
# Example: CODEGRAPH__SERVER__PORT=8080
# Example: CODEGRAPH__LOGGING__LEVEL=debug

# ============================================================================
# LLM Configuration (for local insights generation)
# ============================================================================
# Leave empty to use context-only mode (fastest, recommended for agents like Claude/GPT-4)
# Set to enable local LLM insights generation

# LM Studio with DeepSeek Coder v2 Lite Instruct (or what ever fits in your vGPU memory)
# Supported LLM provider options: "lmstudio", "openai", "anthropic" or "ollama"
# Superior MLX support, Flash Attention 2, KV-cache and Distillation model support on macOS
# CODEGRAPH_LLM_PROVIDER=lmstudio
# LLM_PROVIDER=lmstudio  # Alternative name
# CODEGRAPH_MODEL=lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf
# CODEGRAPH_LMSTUDIO_URL=http://localhost:1234
# CODEGRAPH_CONTEXT_WINDOW=32000 # The context window of the used model directly affects the quality and depth of the results generated by the agentic_ MCP-server tools
# CODEGRAPH_TEMPERATURE=0.1

# Ollama (alternative)
# LLM model (e.g., "qwen3:4b", "Kimi-K2-Instruct")
# CODEGRAPH_MODEL=qwen3:4b
# CODEGRAPH_OLLAMA_URL=http://localhost:11434
# CODEGRAPH_CONTEXT_WINDOW=252000 # The context window of the used model directly affects the quality and depth of the results generated by the agentic_ MCP-server tools

# Anthropic (cloud - 200K/1M tokens)
# CODEGRAPH_LLM_PROVIDER=anthropic
# CODEGRAPH_MODEL=sonnet[1m]
# ANTHROPIC_API_KEY=sk-ant-your-key-here
# CODEGRAPH_CONTEXT_WINDOW=1000000  # 200K/1M tokens

# OpenAI (cloud - 200K/400K tokens)
# CODEGRAPH_LLM_PROVIDER=openai
# CODEGRAPH_MODEL=gpt-5.1-codex-mini
# OPENAI_API_KEY=sk-your-key-here
# OPENAI_ORG_ID=your_fabulous_org
# CODEGRAPH_CONTEXT_WINDOW=200000  # 400K tokens for gpt-5.1 and gpt-5.1-codex
# CODEGRAPH_REASONING_BUDGET=medium

# xAI (cloud - 252K/2M context window, $0.50-$1.50/M tokens!)
# CODEGRAPH_LLM_PROVIDER=openai # OpenAI Responses API compatible providers work
# CODEGRAPH_MODEL=grok-4-1-fast-reasoning  # or grok-code-fast-1 252K tokens the price is same
# XAI_API_KEY=xai-your-key-here
# CODEGRAPH_CONTEXT_WINDOW=2000000  # 2M tokens!
# CODEGRAPH_REASONING_BUDGET=high

# OpenAI compatible providers f.ex. openrouter
# OPENAI_API_BASE=https://openrouter.ai/api/v1
# OPENAI_API_KEY=your-openrouter-api-key
# CODEGRAPH_LLM_PROVIDER=openai

# MCP-server code insights agent max output tokens - uses the CODEGRAPH_MODEL
# MCP_CODE_AGENT_MAX_OUTPUT_TOKENS=52000 # F.ex. Claude Code hard-caps this to 64K tokens and usually crashes is such an large output is produced by an MCP-server

# ============================================================================
# Performance & Caching Configuration (LEGACY only for FAISS+RocksDB)
# ============================================================================

# Performance mode (affects optimization settings)
# CODEGRAPH_PERFORMANCE_MODE=balanced  # Options: high_speed, balanced, low_memory

# Cache configuration
# CODEGRAPH_CACHE_SIZE=1000        # Maximum cache entries
# CODEGRAPH_CACHE_TTL=1800         # Cache TTL in seconds
# CODEGRAPH_ENABLE_CACHE=true

# Symbol indexing batch sizes
# CODEGRAPH_SYMBOL_BATCH_SIZE=500         # Batch size for symbol processing
# CODEGRAPH_SYMBOL_MAX_CONCURRENT=4       # Max concurrent symbol processing
# CODEGRAPH_SYMBOL_DB_BATCH_SIZE=1000     # Batch size for database writes

# ============================================================================
# Server Configuration (LEGACY for REST-API use NAPI instead)
# ============================================================================

# Server host and port (for HTTP/REST API)
# CODEGRAPH_HOST=127.0.0.1
# CODEGRAPH_PORT=3000

# Logging
# -------
# Log level: trace, debug, info, warn, error
# Use "warn" during indexing for clean TUI output (recommended)
# Use "info" for development/debugging
# RUST_LOG=warn

# ============================================================================
# Testing & Development Configuration
# ============================================================================

# Path to codegraph binary (for testing)
# CODEGRAPH_BIN=./target/release/codegraph
# CODEGRAPH_BIN=./target/debug/codegraph

# Command to run codegraph (alternative to binary path)
# CODEGRAPH_CMD="cargo run -p codegraph-mcp --bin codegraph --"

# ============================================================================
# Security Configuration (LEGACY for REST-API)
# ============================================================================

# JWT Authentication
# JWT_SECRET=replace_with_secure_random_secret_minimum_32_characters_long
# JWT_EXPIRY_HOURS=24

# API Key Configuration  
# API_KEY_PREFIX=cgk

# Server Configuration
# HOST=127.0.0.1
# PORT=8080
# ENVIRONMENT=development

# TLS/HTTPS Configuration (for production)
# TLS_CERT_PATH=/path/to/certificate.pem
# TLS_KEY_PATH=/path/to/private-key.pem
# REQUIRE_TLS=true

# Database Configuration
# DATABASE_URL=postgresql://user:password@localhost/codegraph
# REDIS_URL=redis://localhost:6379

# Rate Limiting
# RATE_LIMIT_ANONYMOUS=60
# RATE_LIMIT_USER=1000
# RATE_LIMIT_PREMIUM=5000
# RATE_LIMIT_ADMIN=10000

# Security Settings
# MAX_REQUEST_SIZE=10485760  # 10MB
# SESSION_TIMEOUT_HOURS=24
# PASSWORD_MIN_LENGTH=12

# Logging (see RUST_LOG above for CodeGraph core logging)
# LOG_LEVEL=info  # For application-level logging
# SECURITY_LOG_LEVEL=warn
# LOG_FORMAT=json

# Monitoring
# METRICS_ENABLED=true
# PROMETHEUS_PORT=9090

# External Services
# SENTRY_DSN=https://your-sentry-dsn
# ANALYTICS_KEY=your_analytics_key

# Development/Testing Only
# DEV_MODE=true
# DISABLE_AUTH=false  # Never set to true in production!
# ENABLE_DEBUG_ENDPOINTS=false

# ============================================================================
# MCP Server Configuration (when using --transport http)
# ============================================================================

# Host address to bind HTTP server (default: 127.0.0.1)
# Use 0.0.0.0 to allow external connections
# CODEGRAPH_HTTP_HOST=127.0.0.1

# Port for HTTP server (default: 3003)
# CODEGRAPH_HTTP_PORT=3003

# SSE keep-alive interval in seconds (default: 15)
# Prevents proxy timeouts for long-running agentic operations
# CODEGRAPH_HTTP_KEEP_ALIVE=15

# MCP Test Transport (for test_agentic_mcp.py) can be set by just running codegraph start http --port xxxx or codegraph start stdio
# MCP_TRANSPORT=http     # Use HTTP/SSE transport (requires running server)
# CODEGRAPH_DAEMON_AUTO_START=true # Automatically start daemon when MCP transport is set to http for incremental automatic indexing