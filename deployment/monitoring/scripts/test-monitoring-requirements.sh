#!/bin/bash
# Test script to validate monitoring stack meets all requirements
# Requirements:
# - 95%+ component coverage with Prometheus metrics
# - Grafana dashboards for system health and performance trends
# - ELK stack handling 1000+ log entries/second
# - Health check endpoints detecting issues within 15 seconds
# - Alerting rules triggering within 30 seconds
# - Performance overhead below 2% system impact

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# Test results
TESTS_PASSED=0
TESTS_FAILED=0
TOTAL_TESTS=0

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[PASS]${NC} $1"
    ((TESTS_PASSED++))
    ((TOTAL_TESTS++))
}

log_error() {
    echo -e "${RED}[FAIL]${NC} $1"
    ((TESTS_FAILED++))
    ((TOTAL_TESTS++))
}

log_warning() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

# Test service availability
test_service_availability() {
    log_info "Testing service availability..."
    
    local services=(
        "http://localhost:9090/-/healthy:Prometheus"
        "http://localhost:3000/api/health:Grafana"
        "http://localhost:9200/_cluster/health:Elasticsearch"
        "http://localhost:5601/api/status:Kibana"
        "http://localhost:9093/-/healthy:Alertmanager"
    )
    
    for service in "${services[@]}"; do
        local url="${service%%:*}"
        local name="${service##*:}"
        
        if curl -s --max-time 10 "$url" > /dev/null 2>&1; then
            log_success "$name is available"
        else
            log_error "$name is not responding"
        fi
    done
}

# Test Prometheus metrics coverage (95%+ requirement)
test_prometheus_metrics_coverage() {
    log_info "Testing Prometheus metrics coverage (Target: 95%+)..."
    
    # Expected core metrics for 95%+ coverage
    local expected_metrics=(
        "up"
        "http_requests_total"
        "http_request_duration_seconds"
        "http_requests_in_flight"
        "system_cpu_usage_percent"
        "system_memory_usage_bytes"
        "system_memory_available_bytes"
        "health_check_status"
        "health_check_duration_seconds"
        "graph_nodes_total"
        "graph_edges_total"
        "vector_index_size"
        "vector_search_duration_seconds"
        "parse_operations_total"
        "parse_duration_seconds"
        "connection_pool_active"
        "connection_pool_idle"
        "application_uptime_seconds"
        "build_info"
    )
    
    local available_metrics=0
    local total_expected=${#expected_metrics[@]}
    
    for metric in "${expected_metrics[@]}"; do
        if curl -s "http://localhost:9090/api/v1/query?query=$metric" | jq -e '.data.result | length > 0' > /dev/null 2>&1; then
            ((available_metrics++))
            log_success "Metric '$metric' is available"
        else
            log_error "Metric '$metric' is missing"
        fi
    done
    
    local coverage=$(( available_metrics * 100 / total_expected ))
    if [ $coverage -ge 95 ]; then
        log_success "Metrics coverage: $coverage% (meets 95%+ requirement)"
    else
        log_error "Metrics coverage: $coverage% (below 95% requirement)"
    fi
}

# Test Grafana dashboards
test_grafana_dashboards() {
    log_info "Testing Grafana dashboards..."
    
    # Check if dashboards endpoint is accessible
    if curl -s "http://admin:admin@localhost:3000/api/search?type=dash-db" > /dev/null 2>&1; then
        log_success "Grafana dashboard API is accessible"
    else
        log_error "Cannot access Grafana dashboard API"
        return
    fi
    
    # Check for data source configuration
    if curl -s "http://admin:admin@localhost:3000/api/datasources" | jq -e '. | length > 0' > /dev/null 2>&1; then
        log_success "Grafana data sources are configured"
    else
        log_error "No Grafana data sources configured"
    fi
    
    # Test Prometheus data source connectivity
    if curl -s "http://admin:admin@localhost:3000/api/datasources/proxy/1/api/v1/query?query=up" > /dev/null 2>&1; then
        log_success "Grafana can query Prometheus data source"
    else
        log_error "Grafana cannot query Prometheus data source"
    fi
}

# Test ELK stack high-throughput capability (1000+ logs/second)
test_elk_high_throughput() {
    log_info "Testing ELK stack high-throughput capability (Target: 1000+ logs/second)..."
    
    # Check Elasticsearch cluster health
    local es_health=$(curl -s "http://localhost:9200/_cluster/health" | jq -r '.status')
    if [ "$es_health" = "green" ] || [ "$es_health" = "yellow" ]; then
        log_success "Elasticsearch cluster health: $es_health"
    else
        log_error "Elasticsearch cluster unhealthy: $es_health"
    fi
    
    # Check Elasticsearch settings for high throughput
    local bulk_size=$(curl -s "http://localhost:9200/_cluster/settings" | jq -r '.persistent.bulk_size // "not_set"')
    local refresh_interval=$(curl -s "http://localhost:9200/_all/_settings" | jq -r '.[][] | .settings.index.refresh_interval // "1s"')
    
    log_info "Elasticsearch bulk size setting: $bulk_size"
    log_info "Elasticsearch refresh interval: $refresh_interval"
    
    # Check Logstash pipeline configuration exists
    if [ -f "$(dirname "$0")/../logstash/pipeline/logstash.conf" ]; then
        log_success "Logstash pipeline configuration exists"
        
        # Check for high-throughput settings in Logstash config
        if grep -q "flush_size.*1000" "$(dirname "$0")/../logstash/pipeline/logstash.conf"; then
            log_success "Logstash configured for high-throughput (bulk flush_size: 1000+)"
        else
            log_error "Logstash not optimized for high-throughput"
        fi
    else
        log_error "Logstash pipeline configuration missing"
    fi
    
    # Test log ingestion speed (simulate high load)
    log_info "Simulating high-throughput log ingestion..."
    local start_time=$(date +%s)
    
    # Send 1000 test log entries to check ingestion capacity
    for i in {1..1000}; do
        echo "{\"timestamp\":\"$(date -Iseconds)\",\"level\":\"INFO\",\"message\":\"Test log entry $i\",\"service\":\"load-test\"}" | \
            curl -s -X POST "http://localhost:8080" -H "Content-Type: application/json" -d @- > /dev/null 2>&1 &
        
        # Limit concurrent connections to avoid overwhelming
        if [ $((i % 100)) -eq 0 ]; then
            wait
        fi
    done
    wait
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    local throughput=$((1000 / duration))
    
    if [ $throughput -ge 1000 ]; then
        log_success "Log ingestion throughput: ${throughput} logs/second (meets 1000+ requirement)"
    else
        log_warning "Log ingestion throughput: ${throughput} logs/second (test may not be accurate)"
    fi
}

# Test health check detection time (15-second requirement)
test_health_check_detection_time() {
    log_info "Testing health check detection time (Target: 15 seconds)..."
    
    # Test basic health endpoint response time
    local start_time=$(date +%s%N)
    if curl -s --max-time 10 "http://localhost:3000/health" > /dev/null; then
        local end_time=$(date +%s%N)
        local response_time=$(( (end_time - start_time) / 1000000 )) # Convert to milliseconds
        
        if [ $response_time -lt 1000 ]; then
            log_success "Health check response time: ${response_time}ms (fast)"
        else
            log_warning "Health check response time: ${response_time}ms (slow but acceptable)"
        fi
    else
        log_error "Health check endpoint not responding"
    fi
    
    # Test enhanced health endpoint
    if curl -s --max-time 10 "http://localhost:3000/health/enhanced" > /dev/null; then
        log_success "Enhanced health check endpoint is responsive"
    else
        log_error "Enhanced health check endpoint not responding"
    fi
    
    # Check Prometheus scrape interval (should be 15s or less for 15s detection)
    local scrape_interval=$(curl -s "http://localhost:9090/api/v1/status/config" | jq -r '.data.yaml' | grep -A5 "job_name.*codegraph" | grep "scrape_interval" | head -1 | awk '{print $2}' | tr -d '"')
    
    if [ -n "$scrape_interval" ]; then
        log_info "Prometheus scrape interval: $scrape_interval"
        # Convert to seconds for comparison (supports formats like "5s", "15s")
        local interval_seconds=$(echo "$scrape_interval" | sed 's/s$//')
        if [ "$interval_seconds" -le 15 ]; then
            log_success "Prometheus scrape interval meets 15-second detection requirement"
        else
            log_error "Prometheus scrape interval too long for 15-second detection"
        fi
    else
        log_warning "Could not determine Prometheus scrape interval"
    fi
}

# Test alerting rules response time (30-second requirement)
test_alerting_response_time() {
    log_info "Testing alerting rules response time (Target: 30 seconds)..."
    
    # Check if alerting rules are loaded
    local rules_response=$(curl -s "http://localhost:9090/api/v1/rules")
    if echo "$rules_response" | jq -e '.data.groups | length > 0' > /dev/null; then
        log_success "Alerting rules are loaded in Prometheus"
        
        # Count total rules
        local total_rules=$(echo "$rules_response" | jq '.data.groups[].rules | length' | awk '{sum+=$1} END {print sum}')
        log_info "Total alerting rules configured: $total_rules"
        
        # Check for critical alert rules with 30s or less timing
        local fast_critical_rules=$(echo "$rules_response" | jq '.data.groups[].rules[] | select(.type == "alerting" and .labels.severity == "critical" and (.for // "0s") <= "30s")' | jq -s 'length')
        log_info "Critical rules with â‰¤30s timing: $fast_critical_rules"
        
        if [ "$fast_critical_rules" -gt 0 ]; then
            log_success "Critical alerting rules configured for fast response (â‰¤30s)"
        else
            log_warning "No critical alerting rules found with fast response timing"
        fi
        
    else
        log_error "No alerting rules loaded in Prometheus"
    fi
    
    # Check Alertmanager connectivity
    if curl -s "http://localhost:9093/-/healthy" > /dev/null; then
        log_success "Alertmanager is healthy and can receive alerts"
    else
        log_error "Alertmanager is not responding"
    fi
}

# Test performance overhead (2% requirement)
test_performance_overhead() {
    log_info "Testing monitoring performance overhead (Target: <2% system impact)..."
    
    # Get CPU usage of monitoring containers
    local monitoring_containers=("prometheus" "grafana" "elasticsearch" "kibana" "logstash" "filebeat" "alertmanager" "node-exporter" "cadvisor")
    local total_cpu=0
    local container_count=0
    
    for container in "${monitoring_containers[@]}"; do
        if docker ps --format "{{.Names}}" | grep -q "$container"; then
            local cpu_usage=$(docker stats --no-stream --format "{{.CPUPerc}}" "$container" 2>/dev/null | sed 's/%//' || echo "0")
            if [ -n "$cpu_usage" ] && [ "$cpu_usage" != "0" ]; then
                total_cpu=$(echo "$total_cpu + $cpu_usage" | bc 2>/dev/null || echo "$total_cpu")
                ((container_count++))
                log_info "$container CPU usage: ${cpu_usage}%"
            fi
        fi
    done
    
    if [ $container_count -gt 0 ]; then
        local avg_cpu=$(echo "scale=2; $total_cpu / $container_count" | bc 2>/dev/null || echo "0")
        log_info "Average monitoring overhead: ${avg_cpu}%"
        
        # Compare against 2% threshold
        if [ "$(echo "$avg_cpu < 2.0" | bc 2>/dev/null || echo 0)" -eq 1 ]; then
            log_success "Monitoring overhead ${avg_cpu}% is below 2% requirement"
        else
            log_warning "Monitoring overhead ${avg_cpu}% may exceed 2% requirement"
        fi
    else
        log_warning "Could not measure monitoring container performance"
    fi
    
    # Check memory usage
    local total_memory=0
    for container in "${monitoring_containers[@]}"; do
        if docker ps --format "{{.Names}}" | grep -q "$container"; then
            local mem_usage=$(docker stats --no-stream --format "{{.MemUsage}}" "$container" 2>/dev/null | cut -d'/' -f1 | sed 's/[^0-9.]//g' || echo "0")
            if [ -n "$mem_usage" ] && [ "$mem_usage" != "0" ]; then
                total_memory=$(echo "$total_memory + $mem_usage" | bc 2>/dev/null || echo "$total_memory")
            fi
        fi
    done
    
    log_info "Total monitoring memory usage: ${total_memory}MB"
}

# Test data retention and storage
test_data_retention() {
    log_info "Testing data retention configuration..."
    
    # Check Prometheus retention settings
    local prom_retention=$(curl -s "http://localhost:9090/api/v1/status/config" | jq -r '.data.yaml' | grep -A2 "storage:" | grep "retention_time" | awk '{print $2}')
    if [ -n "$prom_retention" ]; then
        log_success "Prometheus retention configured: $prom_retention"
    else
        log_warning "Could not determine Prometheus retention settings"
    fi
    
    # Check Elasticsearch indices
    local es_indices=$(curl -s "http://localhost:9200/_cat/indices?v" | wc -l)
    if [ "$es_indices" -gt 1 ]; then
        log_success "Elasticsearch indices created: $((es_indices - 1))"
    else
        log_warning "No Elasticsearch indices found"
    fi
}

# Generate test report
generate_report() {
    echo
    echo "=============================================="
    echo "           MONITORING TEST REPORT"
    echo "=============================================="
    echo "Total Tests: $TOTAL_TESTS"
    echo "Passed: $TESTS_PASSED"
    echo "Failed: $TESTS_FAILED"
    echo "Success Rate: $(( TESTS_PASSED * 100 / TOTAL_TESTS ))%"
    echo
    
    if [ $TESTS_FAILED -eq 0 ]; then
        echo -e "${GREEN}ðŸŽ‰ ALL TESTS PASSED - MONITORING STACK MEETS REQUIREMENTS${NC}"
        echo
        echo "âœ… 95%+ Component Coverage with Prometheus"
        echo "âœ… Grafana Dashboards for System Health"
        echo "âœ… ELK Stack High-Throughput Capability"
        echo "âœ… Health Check Detection Within 15 Seconds"
        echo "âœ… Alerting Rules Trigger Within 30 Seconds"
        echo "âœ… Performance Overhead Below 2% System Impact"
        return 0
    else
        echo -e "${RED}âŒ SOME TESTS FAILED - REQUIREMENTS NOT FULLY MET${NC}"
        echo
        echo "Please review failed tests and fix issues before production deployment."
        return 1
    fi
}

# Main test execution
main() {
    echo "=============================================="
    echo "    CODEGRAPH MONITORING REQUIREMENTS TEST"
    echo "=============================================="
    echo "Testing monitoring stack against requirements:"
    echo "â€¢ 95%+ component coverage with Prometheus metrics"
    echo "â€¢ Grafana dashboards for system health and performance trends"
    echo "â€¢ ELK stack handling 1000+ log entries per second"
    echo "â€¢ Health check endpoints detecting issues within 15 seconds"
    echo "â€¢ Alerting rules triggering within 30 seconds"
    echo "â€¢ Performance overhead below 2% system impact"
    echo
    
    # Run all tests
    test_service_availability
    test_prometheus_metrics_coverage
    test_grafana_dashboards
    test_elk_high_throughput
    test_health_check_detection_time
    test_alerting_response_time
    test_performance_overhead
    test_data_retention
    
    # Generate final report
    generate_report
}

# Check if bc is available for calculations
if ! command -v bc &> /dev/null; then
    log_warning "bc (calculator) not available - some calculations may be skipped"
fi

# Check if jq is available for JSON parsing
if ! command -v jq &> /dev/null; then
    log_error "jq is required for testing but not installed"
    exit 1
fi

# Run main function
main "$@"